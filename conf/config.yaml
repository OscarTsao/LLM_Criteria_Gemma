# Hydra configuration for Gemma Encoder training
# Override with: python src/training/train_gemma_hydra.py model.name=google/gemma-3-4b-it

defaults:
  - _self_
  - experiment: null

# Model configuration
model:
  name: google/gemma-3-4b-it
  pooling_strategy: mean  # Options: mean, cls, max, first_k, last_k, attention
  freeze_encoder: false  # Gemma-3 Instruct performs best when fully fine-tuned
  hidden_dropout_prob: 0.1
  classifier_hidden_size: null  # null = simple linear, or specify size like 768
  use_gradient_checkpointing: true  # Enable to reduce memory usage (trades compute for memory)

# Training configuration
training:
  num_epochs: 100
  batch_size: 4  # Safe default for finetuning Gemma-3-4B-IT on 16GB GPUs
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  use_class_weights: true
  early_stopping_patience: 20  # Stop if macro-F1 does not improve for 20 epochs

# Data configuration
data:
  data_dir: ${hydra:runtime.cwd}/data/redsm5
  max_length: 512
  num_folds: 5
  random_seed: 42

# Cross-validation configuration
cv:
  enabled: true
  num_folds: 5
  stratified: true
  save_fold_results: true

# Output configuration
output:
  base_dir: outputs
  experiment_name: gemma3_it_5fold
  save_best_only: true
  save_predictions: true

# Logging
logging:
  log_interval: 10  # steps
  eval_interval: 1  # epochs

# Device configuration
device:
  use_cuda: true
  mixed_precision: true  # Use bfloat16
