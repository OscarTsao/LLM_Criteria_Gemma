# Hydra configuration for Gemma Encoder training
# Override with: python src/training/train_gemma_hydra.py model.name=google/gemma-3-4b-it

defaults:
  - _self_
  - experiment: null

# Model configuration
model:
  name: google/gemma-3-1b-it  # Default: 1B model for 24GB GPUs. Use google/gemma-3-4b-it for 32GB+ GPUs
  pooling_strategy: mean  # Options: mean, cls, max, first_k, last_k, attention
  freeze_encoder: false  # DoRA is used for parameter-efficient fine-tuning
  hidden_dropout_prob: 0.1
  classifier_hidden_size: null  # null = simple linear, or specify size like 768
  use_gradient_checkpointing: true  # Enable to reduce memory usage (trades compute for memory)
  use_flash_attention: true  # Enable Flash Attention via SDPA for faster attention computation
  use_torch_compile: true  # Enable torch.compile (PyTorch 2.0+) for additional speedup (+10-30%)

# Training configuration
training:
  num_epochs: 100
  batch_size: 12  # Maximum safe batch size for RTX 3090 24GB with all optimizations
  gradient_accumulation_steps: 1  # Effective batch = batch_size * accumulation_steps
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  use_class_weights: true
  early_stopping_patience: 20  # Stop if macro-F1 does not improve for 20 epochs
  optimizer_type: adamw_fused  # Options: adamw_fused (CUDA only), adamw (standard)
  early_stopping_min_delta: 0.0  # Minimum improvement delta for early stopping

# Data configuration
data:
  data_dir: ${hydra:runtime.cwd}/data/redsm5
  max_length: 512
  num_folds: 5
  random_seed: 42
  test_size: 0.15  # 15% of data for final test set
  val_size: 0.15   # 15% of remaining data for validation
  post_limit: null  # Limit number of posts (None = use all)
  num_workers: 8  # Maximum workers for async data loading (prevents CPU bottleneck)
  pin_memory: true  # Pin memory for faster GPU transfer (CUDA only)
  prefetch_factor: 4  # Number of batches to prefetch per worker (increased for more workers)
  use_length_bucketing: false  # Enable length bucketing for efficiency (reduces padding waste)
  num_buckets: 10  # Number of length buckets for bucketing sampler

# Cross-validation configuration
cv:
  enabled: true
  num_folds: 5
  stratified: true
  save_fold_results: true

# Output configuration
output:
  base_dir: outputs
  experiment_name: gemma3_1b_5fold  # Updated to reflect 1B model default
  save_best_only: true
  save_predictions: true

# Logging
logging:
  log_interval: 10  # steps
  eval_interval: 1  # epochs

# Device configuration
device:
  use_cuda: true
  mixed_precision: true  # Use bfloat16 (no GradScaler needed)
  use_tf32: true  # Enable TF32 for Ampere GPUs (RTX 3090/A100) - significant speedup

# Runtime fallbacks for constrained environments (e.g., CPU-only)
runtime:
  cpu_debug_mode: true          # Enable automatic downscaling when CUDA unavailable
  cpu_post_limit: 2             # Number of posts to keep during CPU debug mode
  cpu_num_epochs: 1             # Cap epochs for CPU debug mode
  cpu_batch_size: 1             # Safer batch size for CPU debug mode
  force_hf_offline: true        # Skip Hugging Face network calls when offline
  cpu_max_length: 256           # Reduce sequence length for CPU debug mode

# MLflow configuration
mlflow:
  enabled: true
  experiment_name: ${output.experiment_name}  # Use the same experiment name
  tracking_uri: sqlite:///mlflow.db  # SQLite database for tracking
  artifact_location: mlruns  # Directory for storing artifacts
  run_name: null  # Auto-generated from timestamp if null
  tags:  # Additional tags for the run
    model: ${model.name}
    task: binary_nli
  log_models: true  # Log model artifacts to MLflow
  log_model_signatures: true  # Log model input/output signatures
