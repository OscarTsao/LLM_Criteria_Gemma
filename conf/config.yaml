# Hydra configuration for Gemma Encoder training
# Override with: python src/training/train_gemma_hydra.py model.name=google/gemma-3-4b-it

defaults:
  - _self_
  - experiment: null

# Model configuration
model:
  name: google/gemma-3-1b-it  # Default: 1B model for 24GB GPUs. Use google/gemma-3-4b-it for 32GB+ GPUs
  pooling_strategy: mean  # Options: mean, cls, max, first_k, last_k, attention
  freeze_encoder: false  # DoRA is used for parameter-efficient fine-tuning
  hidden_dropout_prob: 0.1
  classifier_hidden_size: null  # null = simple linear, or specify size like 768
  use_gradient_checkpointing: true  # Enable to reduce memory usage (trades compute for memory)

# Training configuration
training:
  num_epochs: 100
  batch_size: 4  # Safe default for Gemma-3-1B-IT on 24GB GPUs with DoRA
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  use_class_weights: true
  early_stopping_patience: 20  # Stop if macro-F1 does not improve for 20 epochs

# Data configuration
data:
  data_dir: ${hydra:runtime.cwd}/data/redsm5
  max_length: 512
  num_folds: 5
  random_seed: 42
  test_size: 0.15  # 15% of data for final test set
  val_size: 0.15   # 15% of remaining data for validation
  post_limit: null  # Limit number of posts (None = use all)

# Cross-validation configuration
cv:
  enabled: true
  num_folds: 5
  stratified: true
  save_fold_results: true

# Output configuration
output:
  base_dir: outputs
  experiment_name: gemma3_1b_5fold  # Updated to reflect 1B model default
  save_best_only: true
  save_predictions: true

# Logging
logging:
  log_interval: 10  # steps
  eval_interval: 1  # epochs

# Device configuration
device:
  use_cuda: true
  mixed_precision: true  # Use bfloat16 (no GradScaler needed)

# Runtime fallbacks for constrained environments (e.g., CPU-only)
runtime:
  cpu_debug_mode: true          # Enable automatic downscaling when CUDA unavailable
  cpu_post_limit: 2             # Number of posts to keep during CPU debug mode
  cpu_num_epochs: 1             # Cap epochs for CPU debug mode
  cpu_batch_size: 1             # Safer batch size for CPU debug mode
  force_hf_offline: true        # Skip Hugging Face network calls when offline
  cpu_max_length: 256           # Reduce sequence length for CPU debug mode

# MLflow configuration
mlflow:
  enabled: true
  experiment_name: ${output.experiment_name}  # Use the same experiment name
  tracking_uri: sqlite:///mlflow.db  # SQLite database for tracking
  artifact_location: mlruns  # Directory for storing artifacts
  run_name: null  # Auto-generated from timestamp if null
  tags:  # Additional tags for the run
    model: ${model.name}
    task: binary_nli
  log_models: true  # Log model artifacts to MLflow
  log_model_signatures: true  # Log model input/output signatures
