# @package _global_
# RTX 3090 Maximum Performance Configuration
# All optimizations enabled and tuned for maximum training speed
# Usage: python src/training/train_nli_binary.py experiment=rtx3090_max_perf

model:
  name: google/gemma-3-1b-it
  pooling_strategy: mean
  use_gradient_checkpointing: true
  use_flash_attention: true
  use_torch_compile: true  # Experimental: +10-30% speedup if stable
  dora_rank: 16
  dora_alpha: 32.0
  dora_dropout: 0.1

training:
  num_epochs: 100
  batch_size: 12  # Maximum safe for 24GB (test 16 if stable)
  learning_rate: 2e-5
  weight_decay: 0.01
  warmup_ratio: 0.1
  max_grad_norm: 1.0
  optimizer_type: adamw_fused
  use_class_weights: true
  early_stopping_patience: 20
  early_stopping_min_delta: 0.0

data:
  data_dir: ${hydra:runtime.cwd}/data/redsm5
  max_length: 512
  test_size: 0.15
  val_size: 0.15
  post_limit: null
  num_workers: 8  # Maximum workers for data loading
  pin_memory: true
  prefetch_factor: 4  # Higher prefetch for more workers

device:
  use_cuda: true
  mixed_precision: true
  use_tf32: true

dora:
  rank: 16
  alpha: 32.0
  dropout: 0.1

mlflow:
  enabled: true
  experiment_name: "nli_binary_rtx3090_max_perf"
  tracking_uri: sqlite:///mlflow.db
  log_models: true

output:
  base_dir: outputs
  experiment_name: nli_binary_rtx3090_max_perf
  save_best_only: true
  save_predictions: true
