version: '3.8'

services:
  # Main training service with GPU support
  train:
    build:
      context: .
      dockerfile: Dockerfile
      target: training
    image: llm-criteria-gemma:train
    container_name: gemma-train
    runtime: nvidia  # Requires nvidia-docker
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    volumes:
      - ./data:/workspace/data
      - ./outputs:/workspace/outputs
      - ./logs:/workspace/logs
      - ./conf:/workspace/conf
    ipc: host
    shm_size: '16gb'
    command: ["python", "src/training/train_gemma_hydra.py"]

  # Development environment with Jupyter
  dev:
    build:
      context: .
      dockerfile: Dockerfile
      target: development
    image: llm-criteria-gemma:dev
    container_name: gemma-dev
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
      - CUDA_VISIBLE_DEVICES=0
    ports:
      - "8888:8888"  # Jupyter
      - "6006:6006"  # TensorBoard
    volumes:
      - .:/workspace
      - ./data:/workspace/data
      - ./outputs:/workspace/outputs
    ipc: host
    shm_size: '16gb'
    stdin_open: true
    tty: true

  # CPU-only version for testing
  test:
    build:
      context: .
      dockerfile: Dockerfile
      target: cpu-only
    image: llm-criteria-gemma:test
    container_name: gemma-test
    volumes:
      - .:/workspace
    command: ["pytest", "tests/", "-v"]

  # Evaluation service
  eval:
    build:
      context: .
      dockerfile: Dockerfile
      target: application
    image: llm-criteria-gemma:eval
    container_name: gemma-eval
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./data:/workspace/data
      - ./outputs:/workspace/outputs
      - ./eval_results:/workspace/eval_results
    command: [
      "python", "src/training/evaluate.py",
      "--checkpoint", "/workspace/outputs/best_model.pt",
      "--output_dir", "/workspace/eval_results"
    ]

  # Benchmark runner
  benchmark:
    build:
      context: .
      dockerfile: Dockerfile
      target: application
    image: llm-criteria-gemma:benchmark
    container_name: gemma-benchmark
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - ./benchmarks:/workspace/benchmarks
    command: [
      "python", "benchmarks/benchmark_poolers.py",
      "--device", "cuda",
      "--iterations", "100"
    ]

networks:
  default:
    name: gemma-network

volumes:
  data:
  outputs:
  logs:
